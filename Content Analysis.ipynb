{"cells":[{"cell_type":"markdown","metadata":{"id":"c8rBGXtZ7aJp"},"source":["# A computational Literature Review of Health psychology Intervention Development\n","\n","## Storm Hiskens-Ravest - 218685427\n","\n","### SIT723 Thesis"]},{"cell_type":"markdown","metadata":{"id":"i5heO_1Jm_2S"},"source":["## Mount Google Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1707376519501,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"},"user_tz":-660},"id":"tg1W63E8ag0-"},"outputs":[],"source":["# # Google Colab\n","# from google.colab import drive\n","# drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{"id":"wRP9RmxZm-Na"},"source":["\n","## Install Libraries\n"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"PTdLCnhnIfGk","executionInfo":{"status":"ok","timestamp":1707375484854,"user_tz":-660,"elapsed":96797,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"outputs":[],"source":["%%capture\n","import os\n","!git clone https://github.com/jupyter/nbconvert.git\n","!cd nbconvert\n","\n","!git clone https://github.com/5H5KN5/SIT723-CLR.git\n","!cd SIT723-CLR\n","os.chdir('SIT723-CLR')\n","\n","!pip install -e .\n","\n","!apt-get install pandoc\n","\n","!apt-get update\n","!apt-get install inkscape\n","!add-apt-repository --yes universe\n","!add-apt-repository --yes ppa:inkscape.dev/stable\n","!apt-get update\n","!apt-get install -y inkscape\n","\n","!apt-get update\n","!apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic texlive-latex-extra -y\n","\n","!pip install optimization\n","!pip install squarify\n","!pip install octis\n","!pip install openai\n","!pip install PyPDF2\n","!pip install keybert\n","!pip install bertopic\n","!pip install tiktoken\n","!pip install transformers\n","!pip install pyspellchecker\n","!pip install sentence-transformers\n","!pip install --upgrade typing_extensions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02HU94Gc7aJs"},"outputs":[],"source":["# Data Manipulation\n","import numpy as np\n","import pandas as pd\n","\n","# PDF Manipulation\n","import PyPDF2\n","\n","# Text Processing and Regular Expressions\n","import re\n","import string\n","\n","# Machine Learning\n","import umap\n","from umap import UMAP\n","import hdbscan\n","from hdbscan import HDBSCAN\n","\n","# Data Visualisation\n","import squarify\n","import seaborn as sns\n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","from typing import List, Union\n","import plotly.graph_objects as go\n","from sklearn.preprocessing import normalize\n","\n","\n","# Natural Language Processing (NLP)\n","import nltk\n","from keybert import KeyBERT\n","from nltk.corpus import stopwords\n","from spellchecker import SpellChecker\n","from nltk.tokenize import sent_tokenize\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from transformers.pipelines import pipeline\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n","\n","# Topic Modeling\n","from bertopic import BERTopic\n","from bertopic.representation import KeyBERTInspired\n","from bertopic.vectorizers import ClassTfidfTransformer\n","\n","# APIs and External Tools Integration\n","import openai\n","from bertopic.representation import OpenAI\n","\n","# Topic Modeling Evaluation\n","from octis.dataset.dataset import Dataset\n","from octis.evaluation_metrics.coherence_metrics import Coherence\n","from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n","\n","# Miscellaneous\n","import os\n","import csv\n","import torch\n","import tiktoken\n","from tqdm import tqdm\n","from typing import Tuple\n","from IPython.display import display"]},{"cell_type":"markdown","metadata":{"id":"H16-_zD3dCW-"},"source":["# Define Classes"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"7Ak6pgUhdB8s","executionInfo":{"status":"ok","timestamp":1707375485437,"user_tz":-660,"elapsed":590,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"outputs":[],"source":["class PDFProcessor:\n","    def __init__(self, pdf_directory, txt_directory):\n","        self.pdf_directory = pdf_directory\n","        self.txt_directory = txt_directory\n","        os.makedirs(self.txt_directory, exist_ok=True)\n","\n","    def extract_text_from_pdf(self, pdf_file_path):\n","        with open(pdf_file_path, 'rb') as file:\n","            pdf_reader = PyPDF2.PdfReader(file)\n","            text = ''.join([page.extract_text() + '\\n' for page in pdf_reader.pages if page.extract_text()])\n","        return text\n","\n","    def save_text_to_file(self, text, filename):\n","        output_path = os.path.join(self.txt_directory, filename.replace('.pdf', '.txt'))\n","        with open(output_path, 'w', encoding='utf-8') as file:\n","            file.write(text)\n","\n","    def process_pdfs(self):\n","        pdf_files = [f for f in os.listdir(self.pdf_directory) if f.endswith('.pdf')]\n","        for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n","            pdf_file_path = os.path.join(self.pdf_directory, pdf_file)\n","            try:\n","                text = self.extract_text_from_pdf(pdf_file_path)\n","                self.save_text_to_file(text, pdf_file)\n","            except Exception as e:\n","                print(f\"Error processing {pdf_file}: {e}\")\n","\n","class TextPreprocessor:\n","    def __init__(self, remove_punctuation=True, punctuation=string.punctuation,\n","                 stopword_list=None, min_chars=2, min_words_docs=1, min_df=0.1,\n","                 max_df=0.8, spellchecker=None):\n","        self.remove_punctuation = remove_punctuation\n","        self.punctuation = punctuation\n","        self.stopword_list = set(stopwords.words('english')) if stopword_list is None else stopword_list\n","        self.min_chars = min_chars\n","        self.min_words_docs = min_words_docs\n","        self.min_df = min_df\n","        self.max_df = max_df\n","        self.spell = spellchecker or SpellChecker()\n","\n","    def extract_relevant_section(self, text):\n","        \"\"\"\n","        Extracts text between 'abstract' and 'references'.\n","        \"\"\"\n","        pattern = r\"\\babstract\\b(.*?)\\breferences\\b\"\n","        match = re.search(pattern, text, flags=re.DOTALL | re.IGNORECASE)\n","        if match:\n","            return match.group(1).strip()  # Extract only the matching group\n","        else:\n","            return text  # Return original text if no match found\n","\n","    def split_merged_words(self, text):\n","        # Split words that are incorrectly merged\n","        words = text.split()\n","        new_text = []\n","        for word in words:\n","            if self.spell.unknown([word]):\n","                for i in range(1, len(word)):\n","                    part1, part2 = word[:i], word[i:]\n","                    if not self.spell.unknown([part1]) and not self.spell.unknown([part2]):\n","                        new_text.extend([part1, part2])\n","                        break\n","                else:\n","                    new_text.append(word)\n","            else:\n","                new_text.append(word)\n","        return ' '.join(new_text)\n","\n","    def recombine_split_words(self, text):\n","        # Logic to recombine incorrectly split words\n","        words = text.split()\n","        new_words = []\n","        temp_word = ''\n","        for word in words:\n","            if len(word) == 1 and word.isalpha():\n","                temp_word += word\n","            else:\n","                if temp_word:\n","                    new_words.append(temp_word)\n","                    temp_word = ''\n","                new_words.append(word)\n","        if temp_word:\n","            new_words.append(temp_word)\n","        return ' '.join(new_words)\n","\n","    def preprocess_text(self, text):\n","        # Initial encoding to ASCII and decoding\n","        text = text.encode('ascii', 'ignore').decode('ascii')\n","\n","        # Remove extra spaces introduced by encoding-decoding\n","        text = re.sub(r'(?<=\\b\\w) (?=\\w\\b)', '', text)\n","        extracted_text = self.extract_relevant_section(text)\n","\n","        # Normalize text, remove punctuation, etc.\n","        # Lowercase conversion moved up before extracting relevant section\n","        text = text.lower()\n","        text = text.translate(str.maketrans('', '', self.punctuation))\n","\n","        # Split into words and remove stopwords\n","        words = text.split()\n","        words = [word for word in words if word not in self.stopword_list and len(word) >= self.min_chars]\n","        text = ' '.join(words)\n","\n","        # Apply the same pre-processing steps to the extracted text\n","        extracted_text = extracted_text.lower()\n","        extracted_text = extracted_text.translate(str.maketrans('', '', self.punctuation))\n","        extracted_words = extracted_text.split()\n","        extracted_words = [word for word in extracted_words if word not in self.stopword_list and len(word) >= self.min_chars]\n","        extracted_text = ' '.join(extracted_words)\n","\n","        # Additional normalization and cleaning for extracted text\n","        extracted_text = re.sub(r'-\\n+', '', extracted_text)\n","        extracted_text = extracted_text.encode('ascii', 'ignore').decode('ascii')\n","        extracted_text = re.sub(r'\\s+', ' ', extracted_text)\n","        extracted_text = re.sub(r'http\\S+|www\\.\\S+', '', extracted_text)\n","        extracted_text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\{.*?\\}', '', extracted_text)\n","\n","        # Replace specific characters\n","        replace_dict = {'&': 'and', 'ﬃ': 'ffi', 'ﬀ': 'ff', 'ﬁ': 'fi', 'ﬂ': 'fl'}\n","        for key, val in replace_dict.items():\n","            extracted_text = extracted_text.replace(key, val)\n","\n","        # Remove all punctuation, including periods\n","        punctuation_to_remove = string.punctuation\n","        extracted_text = extracted_text.translate(str.maketrans('', '', punctuation_to_remove))\n","        extracted_text = extracted_text.lower().translate(str.maketrans('', '', string.digits))\n","\n","        # Remove specific words and phrases\n","        phrases_to_remove = [#r'\\bimplementation\\b',\n","                             #r'\\bstudy\\b',\n","                             #r'\\bresearch\\b'\n","                             #r'\\bhealth\\b',\n","                             r'\\bcopyright\\b',\n","                             r'\\bamerican\\b',\n","                             r'\\bpsychological\\b',\n","                             r'\\bassociation\\b',\n","                             r'\\bauthor\\b',\n","                             #r'\\bintervention\\b',\n","                             #r'\\binterventions\\b',\n","                             #r'\\bpsychology\\b',\n","                             r'\\bissn\\b',\n","                             r'\\bprint\\b',\n","                             r'\\bbackground\\b',\n","                             r'\\bfull\\b',\n","                             r'\\bterms\\b',\n","                             r'\\bconditions\\b',\n","                             r'\\baccess\\b',\n","                             r'\\buse\\b',\n","                             r'\\bfound\\b',\n","                             r'\\breview\\b',\n","                             r'\\bjournal\\b']\n","\n","        for phrase in phrases_to_remove:\n","            extracted_text = re.sub(phrase, '', extracted_text, flags=re.IGNORECASE)\n","\n","        return extracted_text\n","\n","    def process_documents(self, docs, filenames, preprocessed_txt_path):\n","        for i in tqdm(range(len(docs)), desc=\"Processing Documents\"):\n","            preprocessed_text = self.preprocess_text(docs[i])\n","            filename = filenames[i]\n","            output_path = os.path.join(preprocessed_txt_path, filename)\n","            with open(output_path, 'w', encoding='utf-8') as file:\n","                file.write(preprocessed_text)\n","\n","class DataLoader:\n","    \"\"\"\n","    A class for loading text data from a directory and extracting metadata.\n","\n","    Attributes:\n","        directory (str): The directory containing text files.\n","    \"\"\"\n","\n","    def __init__(self, directory):\n","        \"\"\"\n","        Initialises a DataLoader object with the specified directory.\n","\n","        Args:\n","            directory (str): The directory containing text files.\n","        \"\"\"\n","        self.directory = directory\n","\n","    def extract_year_from_filename(self, filename):\n","        \"\"\"\n","        Extracts the year from a filename with a specific format.\n","\n","        Args:\n","            filename (str): The filename from which to extract the year.\n","\n","        Returns:\n","            str or None: The extracted year or None if not found.\n","        \"\"\"\n","        match = re.search(r'_\\d{4}_', filename)\n","        return match.group(0)[1:-1] if match else None\n","\n","    def load_texts(self):\n","        \"\"\"\n","        Loads text data from files in the specified directory.\n","\n","        Returns:\n","            list of str: A list of text content from the loaded files.\n","        \"\"\"\n","        texts = []\n","        file_list = [f for f in os.listdir(self.directory) if f.endswith('.txt')]\n","        for filename in tqdm(file_list, desc='Loading files'):\n","            with open(os.path.join(self.directory, filename), 'r', encoding='utf-8') as file:\n","                texts.append(file.read())\n","        return texts\n","\n","    def create_corpus_file(self, output_dir, filename=\"corpus.tsv\"):\n","        \"\"\"\n","        Creates a corpus file in a format suitable for OCTIS.\n","\n","        Args:\n","            output_dir (str): The directory to save the corpus file.\n","            filename (str): The name of the corpus file.\n","        \"\"\"\n","        file_path = os.path.join(output_dir, filename)\n","        with open(file_path, 'w', encoding='utf-8') as file:\n","            file_list = [f for f in os.listdir(self.directory) if f.endswith('.txt')]\n","            for filename in tqdm(file_list, desc='Creating corpus file'):\n","                year = self.extract_year_from_filename(filename)\n","                with open(os.path.join(self.directory, filename), 'r', encoding='utf-8') as text_file:\n","                    content = text_file.read().strip()\n","                    file.write(f\"{content}\\t{year}\\n\")\n","\n","    def create_vocabulary_file(self, output_dir, filename=\"vocabulary.txt\"):\n","        \"\"\"\n","        Creates a vocabulary file from the texts in the specified directory.\n","\n","        Args:\n","            output_dir (str): The directory to save the vocabulary file.\n","            filename (str): The name of the vocabulary file.\n","        \"\"\"\n","        vocab = collections.Counter()\n","        file_list = [f for f in os.listdir(self.directory) if f.endswith('.txt')]\n","        for filename in tqdm(file_list, desc='Building vocabulary'):\n","            with open(os.path.join(self.directory, filename), 'r', encoding='utf-8') as text_file:\n","                words = text_file.read().split()\n","                vocab.update(words)\n","\n","        vocab_path = os.path.join(output_dir, filename)\n","        with open(vocab_path, 'w', encoding='utf-8') as vocab_file:\n","            for word in sorted(vocab):\n","                vocab_file.write(word + '\\n')\n","    def write_documents_to_tsv(self, tsv_path, documents):\n","        \"\"\"\n","        Writes the documents to a TSV file.\n","\n","        Args:\n","            tsv_path (str): The path to the TSV file where documents will be written.\n","            documents (list of str): A list of documents to be written to the TSV file.\n","        \"\"\"\n","        with open(tsv_path, 'w', encoding='utf-8') as tsv_file:\n","            for idx, document in enumerate(documents):\n","                cleaned_document = document.replace('\\t', ' ').replace('\\n', ' ')\n","                tsv_file.write(cleaned_document + '\\n')\n","                if '\\t' in document or '\\n' in document:\n","                    print(f\"Warning: Document at index {idx} contained tabs or newlines and was cleaned.\")\n","\n","class EmbeddingModel:\n","    \"\"\"\n","    A class for generating embeddings from text data using a pre-trained model.\n","\n","    Attributes:\n","        model_name (str): The name of the pre-trained embedding model.\n","    \"\"\"\n","\n","    def __init__(self, model_name):\n","        \"\"\"\n","        Initializes an EmbeddingModel object with the specified pre-trained model name.\n","\n","        Args:\n","            model_name (str): The name of the pre-trained embedding model.\n","        \"\"\"\n","        self.model = SentenceTransformer(model_name)\n","\n","    def generate_embeddings(self, documents, is_sentences=False):\n","        \"\"\"\n","        Generates embeddings for a list of documents or sentences.\n","\n","        Args:\n","            documents (list of str): The list of documents or sentences to generate embeddings for.\n","            is_sentences (bool): True if the input is a list of sentences, False if it's a list of documents.\n","\n","        Returns:\n","            list of numpy.ndarray: A list of embeddings for the input documents or sentences.\n","        \"\"\"\n","        if is_sentences:\n","            sentences = [sentence for doc in documents for sentence in sent_tokenize(doc)]\n","            return self.model.encode(sentences, show_progress_bar=True)\n","        return self.model.encode(documents, show_progress_bar=True)\n","\n","class UMAPDimensionalityReduction:\n","    def __init__(self, n_neighbors, n_components, min_dist, metric, random_state=42):\n","        self.umap_model = UMAP(n_neighbors=n_neighbors,\n","                               n_components=n_components,\n","                               min_dist=min_dist,\n","                               metric=metric,\n","                               random_state=random_state)\n","\n","    def fit_transform(self, embeddings):\n","        return self.umap_model.fit_transform(embeddings)\n","\n","\n","    def fit_hdbscan_model(self, embeddings):\n","        \"\"\"\n","        Fits the HDBSCAN clustering model to embeddings.\n","\n","        Args:\n","            embeddings (list of numpy.ndarray): The embeddings on which HDBSCAN will perform clustering.\n","\n","        Returns:\n","            HDBSCAN: The fitted HDBSCAN clustering model.\n","        \"\"\"\n","        return self.hdbscan_model.fit(embeddings)\n","\n","class HDBSCANClustering:\n","    def __init__(self, min_cluster_size, metric, cluster_selection_method, prediction_data=True):\n","        self.hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size,\n","                                     metric=metric,\n","                                     cluster_selection_method=cluster_selection_method,\n","                                     prediction_data=prediction_data)\n","\n","    def fit(self, embeddings):\n","        return self.hdbscan_model.fit(embeddings)\n","\n","class TopicModeling:\n","    \"\"\"\n","    A class for performing topic modeling using BERTopic and related components.\n","\n","    Attributes:\n","        embedding_model: The embedding model for text data.\n","        umap_model: The UMAP dimensionality reduction model.\n","        hdbscan_model: The HDBSCAN clustering model.\n","    \"\"\"\n","    def __init__(self, embedding_model, umap_model, hdbscan_model, top_n_words, nr_topics, filenames=None):\n","        \"\"\"\n","        Initializes a TopicModeling object with the specified components.\n","\n","        Args:\n","            embedding_model: The embedding model for text data.\n","            umap_model: The UMAP dimensionality reduction model.\n","            hdbscan_model: The HDBSCAN clustering model.\n","            top_n_words (int): The number of top words for each topic in BERTopic.\n","            nr_topics (int): The number of topics to extract in BERTopic.\n","            doc_paths (list of str, optional): The paths of documents indexed corresponding to the input documents.\n","        \"\"\"\n","        self.vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n","        self.representation_model = KeyBERT()\n","        self.topic_model = BERTopic(\n","            calculate_probabilities=True,\n","            embedding_model=embedding_model,\n","            umap_model=umap_model,\n","            hdbscan_model=hdbscan_model,\n","            vectorizer_model=self.vectorizer_model,\n","            representation_model=self.representation_model,\n","            top_n_words=top_n_words,\n","            nr_topics=nr_topics,\n","            verbose=True\n","        )\n","        self.filenames = filenames  # Store document paths\n","\n","    def fit_transform(self, documents, embeddings=None):\n","        \"\"\"\n","        Fits the BERTopic model to documents, optionally using precomputed embeddings.\n","\n","        Args:\n","            documents (list of str): The input documents.\n","            embeddings (list of numpy.ndarray, optional): Precomputed embeddings for the input documents.\n","\n","        Returns:\n","            tuple: A tuple containing topics and probabilities. If embeddings are provided, uses those; otherwise, generates embeddings.\n","        \"\"\"\n","        if embeddings is None:\n","            embeddings = self.model.encode(documents, show_progress_bar=True)\n","        topics, probabilities = self.topic_model.fit_transform(documents, embeddings)\n","        self.doc_topics = topics\n","        return topics, probabilities\n","\n","    def get_topic_info(self):\n","        \"\"\"\n","        Gets information about topics including words and probabilities.\n","\n","        Returns:\n","            pandas.DataFrame: A DataFrame containing topic information.\n","        \"\"\"\n","        topic_info_df = self.topic_model.get_topic_info()\n","        topic_info_df['representative_filenames'] = topic_info_df['Topic'].apply(lambda topic: [self.filenames[i] for i, t in enumerate(self.doc_topics) if t == topic])\n","        return topic_info_df\n","\n","    def fit_hierarchical_topics(self, documents):\n","        \"\"\"\n","        Fits hierarchical topics to the input documents.\n","\n","        Args:\n","            documents (list of str): The input documents.\n","\n","        Returns:\n","            str: A string representation of the hierarchical topics.\n","        \"\"\"\n","        return self.topic_model.hierarchical_topics(documents)\n","\n","    def topics_over_time(self, documents, timestamps, nr_bins):\n","        \"\"\"\n","        Performs dynamic topic modeling.\n","\n","        Args:\n","            documents (list of str): The input documents.\n","            timestamps (list of str): Timestamps corresponding to each document.\n","            topics (list of int): The topics assigned to each document.\n","            nr_bins (int): The number of bins to divide the timestamps.\n","\n","        Returns:\n","            DataFrame: A DataFrame containing dynamic topics over time.\n","        \"\"\"\n","        return self.topic_model.topics_over_time(documents, timestamps, nr_bins=nr_bins)\n","\n","    def reduce_outliers(self, documents, topics, probabilities=None, strategy=\"probabilities\"):\n","\n","        \"\"\"\n","        Reduces outliers using the specified strategy.\n","\n","        Args:\n","            documents (list of str): The documents to process.\n","            topics (list of int): The initial topics assigned to each document.\n","            probabilities (list of float, optional): The probabilities of each document belonging to its topic.\n","            strategy (str): The strategy to use for reducing outliers. Options are 'probabilities', 'distributions',\n","                            'c-tf-idf', 'embeddings'.\n","\n","        Returns:\n","            list of int: The updated topics with reduced outliers.\n","        \"\"\"\n","        new_topics = []\n","\n","        if strategy == \"probabilities\":\n","            if probabilities is None:\n","                raise ValueError(\"Probabilities must be provided for the 'probabilities' strategy.\")\n","            new_topics = self.topic_model.reduce_outliers(documents, topics, probabilities=probabilities, strategy=strategy)\n","\n","        return new_topics\n","\n","    def merge_topics(self, documents, topics_to_merge):\n","        \"\"\"\n","        Merges topics in the BERTopic model.\n","\n","        Args:\n","            documents (list of str): The documents used in topic modeling.\n","            topics_to_merge (list of list of int): A list where each element is a list containing two topic numbers to be merged.\n","        \"\"\"\n","        # Check if the BERTopic instance has the merge_topics method\n","        if hasattr(self.topic_model, 'merge_topics'):\n","            for topics_list in topics_to_merge:\n","                # Ensure there's more than one topic in the list to merge\n","                if len(topics_list) > 1:\n","                    # The first topic in the list is the target topic we merge other topics into\n","                    target_topic = topics_list[0]\n","                    # Topics to be merged into the target topic, excluding the target topic itself\n","                    topics_to_be_merged = topics_list[1:]\n","\n","                    # Merge each topic into the target topic\n","                    for topic in topics_to_be_merged:\n","                        # BERTopic's merge_topics method might be expecting the target topic and a single topic to merge at a time\n","                        # You may need to adjust this call depending on the exact requirements of BERTopic's merge_topics method\n","                        self.topic_model.merge_topics(documents, [target_topic, topic])\n","                else:\n","                    print(f\"Invalid topics list for merging: {topics_list}\")\n","        else:\n","            print(\"The BERTopic instance does not support topic merging.\")\n","\n","    def update_topics(self, documents, new_topics):\n","        \"\"\"\n","        Updates the BERTopic model with new topic assignments.\n","\n","        Args:\n","            documents (list of str): The documents used in topic modeling.\n","            new_topics (list of int): The new topic assignments for each document.\n","        \"\"\"\n","        self.topic_model.update_topics(documents, topics=new_topics, n_gram_range=(1, 3))\n","\n","class TopicVisualisation:\n","    \"\"\"\n","    A class dedicated to visualising topics and their relationships.\n","    \"\"\"\n","    def __init__(self, topic_model):\n","        self.topic_model = topic_model\n","\n","    def visualise_topics(self):\n","        \"\"\"\n","        Visualises the topics generated by the topic model.\n","\n","        Returns:\n","            plotly.Figure: A figure displaying the visualisation of topics.\n","        \"\"\"\n","        topic_info = self.topic_model.get_topic_info()\n","        if topic_info.shape[0] > 1:  # Check if there are topics\n","            return self.topic_model.visualize_topics()\n","        else:\n","            print(\"No topics to visualize.\")\n","            return None\n","\n","    def visualise_barchart(self, top_n_topics):\n","        \"\"\"\n","        Visualises a bar chart of the top topics generated by the topic model.\n","\n","        Args:\n","            top_n_topics (int, optional): The number of top topics to visualise.\n","\n","        Returns:\n","            plotly.Figure: A figure displaying the bar chart.\n","        \"\"\"\n","        return self.topic_model.visualize_barchart(top_n_topics=top_n_topics)\n","\n","    def visualise_hierarchy(self, hierarchical_topics):\n","        \"\"\"\n","        Visualises hierarchical topics.\n","\n","        Args:\n","            hierarchical_topics: Hierarchical topics data.\n","\n","        Returns:\n","            plotly.Figure: A figure displaying the hierarchical topics visualization.\n","        \"\"\"\n","        return self.topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n","\n","    def visualise_heatmap(self):\n","        \"\"\"\n","        Visualises a heatmap of the topic model.\n","\n","        Returns:\n","            plotly.Figure: A figure displaying the heatmap.\n","        \"\"\"\n","        return self.topic_model.visualize_heatmap()\n","\n","    def visualise_topics_over_time(self, topics_over_time: pd.DataFrame, top_n_topics: int = None,\n","                                   topics: List[int] = None, normalize_frequency: bool = False,\n","                                   custom_labels: Union[bool, str] = False, title: str = \"<b>Topics over Time</b>\",\n","                                   width: int = 1500, height: int = 800, colormap: str = 'Viridis') -> go.Figure:\n","        \"\"\"\n","        Visualize topics over time using labels from a dictionary, excluding topic -1,\n","        and allowing any Plotly colormap.\n","        \"\"\"\n","        # Handle Plotly's built-in color scales if a string is provided\n","        if isinstance(colormap, str):\n","            try:\n","                # Attempt to fetch the colormap by name from Plotly Express\n","                colors = getattr(px.colors.sequential, colormap)\n","            except AttributeError:\n","                # If colormap is not found, default to Viridis\n","                colors = px.colors.sequential.Viridis\n","        else:\n","            # If a list is provided, use it directly as the colormap\n","            colors = colormap\n","\n","        # Exclude topic -1 from visualization\n","        data = topics_over_time[topics_over_time['Topic'] != -1].copy()\n","\n","        # If specific topics are requested, filter by those; otherwise, select top N topics if specified\n","        if topics is not None:\n","            data = data[data['Topic'].isin(topics)]\n","        elif top_n_topics is not None:\n","            top_topics = data['Topic'].value_counts().head(top_n_topics).index\n","            data = data[data['Topic'].isin(top_topics)]\n","\n","        # Initialize Plotly figure\n","        fig = go.Figure()\n","\n","        # Iterate over topics to create traces\n","        for index, topic in enumerate(sorted(data['Topic'].unique(), key=lambda x: (x != -1, x))):\n","            topic_data = data[data['Topic'] == topic]\n","            label = topic_labels.get(topic, f\"Topic {topic}\")  # Get label from dictionary\n","\n","            # Normalize frequency if requested\n","            if normalize_frequency:\n","                y = normalize(topic_data['Frequency'].values.reshape(1, -1))[0].tolist()\n","            else:\n","                y = topic_data['Frequency'].values\n","\n","            # Determine the color for the trace\n","            color = colors[index % len(colors)]\n","\n","            # Add trace to the figure for this topic\n","            fig.add_trace(go.Scatter(x=topic_data['Timestamp'], y=y, mode='lines+markers',\n","                                     marker_color=color, hoverinfo=\"text\",\n","                                     name=label,  # Use label for legend\n","                                     hovertext=[f'<b>{label}</b><br>Words: {words}' for words in topic_data['Words']]))\n","\n","        # Update layout with legend on the right\n","        fig.update_layout(\n","            yaxis_title=\"Frequency\",\n","            title={'text': title, 'y': 0.9, 'x': 0.5, 'xanchor': 'center', 'yanchor': 'top'},\n","            template=\"plotly_white\", width=width, height=height,\n","            hoverlabel=dict(bgcolor=\"white\", font_size=12, font_family=\"Rockwell\"),\n","            legend=dict(\n","                title=\"<b>Legend</b>\",\n","                orientation=\"v\",\n","                yanchor=\"middle\",\n","                y=0.5,\n","                xanchor=\"left\",\n","                x=1.05\n","            )\n","        )\n","        return fig\n","\n","class TopicModelEvaluation:\n","    \"\"\"\n","    A class for evaluating topic models focusing on NPMI coherence, topic diversity,\n","    and KL background metrics.\n","    \"\"\"\n","    def __init__(self, topic_model, dataset_path):\n","        \"\"\"\n","        Initializes the TopicModelEvaluation object with a topic model and dataset path.\n","\n","        Args:\n","            topic_model: The topic model to be evaluated.\n","            dataset_path (str): The path to the OCTIS dataset for evaluation.\n","        \"\"\"\n","        self.topic_model = topic_model\n","        self.dataset_path = dataset_path\n","        self.octis_dataset = self._load_dataset()\n","\n","    def _load_dataset(self):\n","        \"\"\"\n","        Loads the dataset from the provided dataset path.\n","\n","        Returns:\n","            octis_dataset: The OCTIS dataset object loaded from the dataset path.\n","        \"\"\"\n","        octis_dataset = Dataset()\n","        octis_dataset.load_custom_dataset_from_folder(self.dataset_path)\n","        return octis_dataset\n","\n","    def evaluate_npmi_coherence(self, topk):\n","        \"\"\"\n","        Evaluates the topic model using the NPMI coherence metric.\n","\n","        Args:\n","            topk (int, optional): The number of top words to consider for coherence calculation.\n","\n","        Returns:\n","            float: The NPMI coherence score.\n","        \"\"\"\n","        topics_for_octis = [[word for word, _ in self.topic_model.get_topic(topic_id)[:topk]] for topic_id in self.topic_model.get_topics().keys()]\n","        model_output = {\"topics\": topics_for_octis}\n","        coherence_metric = Coherence(texts=self.octis_dataset.get_corpus(), topk=topk, measure='c_npmi')\n","        npmi_score = coherence_metric.score(model_output)\n","        return npmi_score\n","\n","    def evaluate_topic_diversity(self, topk=5):\n","        \"\"\"\n","        Evaluates the topic diversity of the model.\n","\n","        Args:\n","            topk (int, optional): The number of top words to consider for diversity calculation.\n","\n","        Returns:\n","            float: The topic diversity score.\n","        \"\"\"\n","        topics = [[word for word, _ in self.topic_model.get_topic(topic_id)[:topk]] for topic_id in self.topic_model.get_topics().keys()]\n","        model_output = {\"topics\": topics}\n","        topic_diversity = TopicDiversity(topk=topk)\n","        diversity_score = topic_diversity.score(model_output)\n","        return diversity_score\n","\n","    def evaluate_model(self, topk):\n","        \"\"\"\n","        Performs a comprehensive evaluation using NPMI coherence, topic diversity.\n","\n","        Args:\n","            topk (int, optional): The number of top words to consider for evaluation.\n","\n","        Returns:\n","            dict: A dictionary containing scores for each of the evaluated metrics.\n","        \"\"\"\n","        npmi_score = self.evaluate_npmi_coherence(topk)\n","        diversity_score = self.evaluate_topic_diversity(topk)\n","\n","        return {\n","            \"NPMI Coherence\": npmi_score,\n","            \"Topic Diversity\": diversity_score,\n","        }\n","\n","    def evaluate_individual_topics(self, topk):\n","        \"\"\"\n","        Evaluates individual topics generated by the topic model using OCTIS metrics.\n","\n","        Args:\n","            topk (int): The number of top words to consider for coherence calculation.\n","\n","        Returns:\n","            dict: A dictionary containing OCTIS coherence scores for each topic.\n","        \"\"\"\n","        extracted_topics = self.topic_model.get_topics()\n","        topics_for_octis = {topic_id: [word for word, _ in self.topic_model.get_topic(topic_id)[:topk]]\n","                            for topic_id in extracted_topics.keys()}\n","\n","        individual_coherence_scores = {}\n","        for topic_id, words in topics_for_octis.items():\n","            model_output = {\"topics\": [words[:topk]]}  # Ensure words list has at least 'topk' elements\n","            npmi_octis = Coherence(texts=self.octis_dataset.get_corpus(), topk=len(words[:topk]), measure=\"c_npmi\")\n","            individual_coherence_scores[topic_id] = npmi_octis.score(model_output)\n","\n","        return individual_coherence_scores"]},{"cell_type":"markdown","metadata":{"id":"np_lxn7sOVeo"},"source":["## Set Paths and Load Data"]},{"cell_type":"code","source":["pdf_path = '/content/SIT723-CLR/Data/Content/Articles/PDF'\n","txt_path = '/content/SIT723-CLR/Data/Content/Articles/txt'\n","raw_dataset_path = '/content/SIT723-CLR/Data/Content/Articles/txt'\n","dataset_path = '/content/SIT723-CLR/Data/Content/Articles/txt/preprocessed'\n","tsv_directory = '/content/SIT723-CLR/Data/Content/Articles/tsv'\n","tsv_path = '/content/SIT723-CLR/Data/Content/Articles/tsv/corpus.tsv'"],"metadata":{"id":"Rp6TJmnrRf4z","executionInfo":{"status":"ok","timestamp":1707375485437,"user_tz":-660,"elapsed":8,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"id":"sQOrCoq-7Pmd","executionInfo":{"status":"ok","timestamp":1707375485438,"user_tz":-660,"elapsed":8,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"outputs":[],"source":["# # Set Paths\n","# pdf_path = '/content/drive/My Drive/Colab Notebooks/SIT723/Data/Articles/PDF'\n","# txt_path = '/content/drive/My Drive/Colab Notebooks/SIT723/Data/Articles/txt'\n","# raw_dataset_path = '/content/drive/My Drive/Colab Notebooks/SIT723/Data/Articles/txt'\n","# dataset_path = '/content/drive/My Drive/Colab Notebooks/SIT723/Data/Articles/txt/preprocessed'\n","# tsv_directory = '/content/drive/My Drive/Colab Notebooks/SIT723/Data/Articles/tsv'\n","# tsv_path = '/content/drive/My Drive/Colab Notebooks/SIT723/Data/Articles/tsv/corpus.tsv'"]},{"cell_type":"markdown","metadata":{"id":"yuDNzov0G56-"},"source":["## Extract Text from PDFs"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"fiQ9nMfdG40r","executionInfo":{"status":"ok","timestamp":1707375485438,"user_tz":-660,"elapsed":8,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"outputs":[],"source":["# # Extract Text from PDF\n","# pdf_processor = PDFProcessor(pdf_path, txt_path)\n","# pdf_processor.process_pdfs()"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"q7fD_TP4CLbK","executionInfo":{"status":"ok","timestamp":1707375485438,"user_tz":-660,"elapsed":8,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"outputs":[],"source":["# Initialize empty lists to store text content and filenames\n","docs, filenames = [], []\n","\n","# Iterate through files in the specified directory\n","for filename in os.listdir(txt_path):\n","    # Check if the file has a \".txt\" extension\n","    if filename.endswith(\".txt\"):\n","        # Construct the full file path\n","        filepath = os.path.join(txt_path, filename)\n","        # Open the file for reading\n","        with open(filepath, 'r') as file:\n","            # Append the file's content to the docs list\n","            docs.append(file.read())\n","            # Append the filename to the filenames list\n","            filenames.append(filename)"]},{"cell_type":"markdown","metadata":{"id":"YFf7dVEvCRee"},"source":["## Load Raw .txt Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RwClXuBLCM4r"},"outputs":[],"source":["# Loading raw txt files\n","data_loader = DataLoader(directory=raw_dataset_path)\n","raw_documents = data_loader.load_texts()"]},{"cell_type":"markdown","metadata":{"id":"-5OljxaFCO1D"},"source":["## Preprocess Raw .txt Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgtXEsdHCOSr"},"outputs":[],"source":["preprocessor = TextPreprocessor(\n","    remove_punctuation=True,\n","    punctuation=string.punctuation,\n","    stopword_list=set(stopwords.words('english')),\n","    min_chars=4,\n","    min_words_docs=1,\n","    #min_df=0.1,\n","    #max_df=0.8,\n","    spellchecker=SpellChecker()\n",")\n","\n","preprocessor.process_documents(docs, filenames, dataset_path)"]},{"cell_type":"markdown","metadata":{"id":"79isRDhvC9Sd"},"source":["## Load Preprocessed Documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-t_eDo9lC18W"},"outputs":[],"source":["# Loading preprocessed txt files\n","# Initialize DataLoader\n","data_loader = DataLoader(directory=dataset_path)\n","\n","# Load preprocessed documents\n","preprocessed_documents = data_loader.load_texts()\n","\n","documents = preprocessed_documents\n","\n","# Write preprocessed documents to TSV\n","data_loader.write_documents_to_tsv(tsv_path, preprocessed_documents)\n","\n","# Loading tsv file\n","dataset = Dataset()\n","dataset.load_custom_dataset_from_folder(tsv_directory)"]},{"cell_type":"code","source":["# Initialize empty lists to store text content and filenames\n","docs, filenames = [], []\n","\n","# Iterate through files in the specified directory\n","for filename in os.listdir(dataset_path):\n","    # Check if the file has a \".txt\" extension\n","    if filename.endswith(\".txt\"):\n","        # Construct the full file path\n","        filepath = os.path.join(dataset_path, filename)\n","        # Open the file for reading\n","        with open(filepath, 'r') as file:\n","            # Append the file's content to the docs list\n","            docs.append(file.read())\n","            # Append the filename to the filenames list\n","            filenames.append(filename)"],"metadata":{"id":"HQBgF94nY3rh","executionInfo":{"status":"ok","timestamp":1707375508180,"user_tz":-660,"elapsed":655,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9tAfD2KWnYCT"},"source":["## Generate Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90XzgXhOjRBm"},"outputs":[],"source":["# Generate Embeddings\n","embedding_model_instance = EmbeddingModel(model_name='all-mpnet-base-v2')\n","embeddings = embedding_model_instance.generate_embeddings(documents, is_sentences=True)"]},{"cell_type":"markdown","metadata":{"id":"yx7iDD5Qnc7M"},"source":["## Initialise Dimension Reduction and Clustering Models"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"IocR861ZmChj","executionInfo":{"status":"ok","timestamp":1707375562490,"user_tz":-660,"elapsed":3966,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"outputs":[],"source":["# Apply UMAP Dimensionality Reduction on Embeddings\n","umap_instance = UMAPDimensionalityReduction(n_neighbors=5,\n","                                            n_components=2,\n","                                            min_dist=1,\n","                                            metric='euclidean')\n","umap_embeddings = umap_instance.fit_transform(embeddings)\n","\n","# Apply HDBSCAN Clustering on Reduced Embeddings\n","hdbscan_instance = HDBSCANClustering(min_cluster_size=5,\n","                                     metric='euclidean',\n","                                     cluster_selection_method='leaf')\n","clusters = hdbscan_instance.fit(umap_embeddings)"]},{"cell_type":"markdown","metadata":{"id":"XS7_EGl0nia8"},"source":["## Fit BERTopic Topic Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5W17c1Sb9tg"},"outputs":[],"source":["# Initialize TopicModeling with custom parameters\n","topic_modeling = TopicModeling(\n","    embedding_model_instance.model,\n","    umap_instance.umap_model,\n","    hdbscan_instance.hdbscan_model,\n","    top_n_words=5,\n","    nr_topics=None,  # Dynamic number of topics\n","    filenames=filenames\n",")\n","topics, probabilities = topic_modeling.fit_transform(documents, umap_embeddings)"]},{"cell_type":"markdown","metadata":{"id":"G0jEtG8fnlyb"},"source":["## View Topics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wBFlnZg8D_y"},"outputs":[],"source":["# Retrieve and Print Topic Info\n","topic_info = topic_modeling.get_topic_info()\n","print(topic_info)\n","\n","topic_info = topic_info.drop(columns=['Representative_Docs'])\n","\n","# Save the modified DataFrame to a CSV file\n","topic_info.to_csv('/content/SIT723-CLR/Results/csv/topic_info-1.csv', index=False)"]},{"cell_type":"code","source":["# Iterate through each topic in the DataFrame\n","for index, row in topic_info.iterrows():\n","    topic_number = row['Topic']  # Assuming there's a 'Topic' column\n","    representative_filenames = row['representative_filenames']  # This is the column you added\n","\n","    # Print the topic number\n","    print(f\"Topic {topic_number} representative filenames:\")\n","\n","    # Iterate through filenames and print each on a separate line\n","    for filename in representative_filenames:\n","        print(f\"    {filename}\")\n","\n","    # Add a blank line for better readability between topics\n","    print()"],"metadata":{"id":"XOZrLkdwaVE8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2HJBJ2tn05u"},"source":["## Evaluate Topic Model Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhxEsDT98I_C"},"outputs":[],"source":["# Evaluation\n","# Create an instance of TopicModelEvaluation\n","evaluation = TopicModelEvaluation(topic_model=topic_modeling.topic_model, dataset_path=tsv_directory)\n","\n","# Call evaluate_model on the instance\n","scores = evaluation.evaluate_model(topk=5)\n","\n","# Print the evaluation scores\n","print(\"\\nTopic Model Evaluation:\")\n","print(scores)"]},{"cell_type":"code","source":["# Evaluating individual topics\n","evaluation = TopicModelEvaluation(topic_model=topic_modeling.topic_model, dataset_path=tsv_directory)\n","individual_coherence_scores = evaluation.evaluate_individual_topics(topk=5)\n","\n","# Print individual coherence results\n","print(\"\\nIndividual Topic Coherence Scores (NPMI):\")\n","for topic_id, score in individual_coherence_scores.items():\n","    print(f\"Topic {topic_id}: {score}\")"],"metadata":{"id":"Lp3Xpue_cAZW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5YbxLCZorlvJ"},"source":["## Outlier Reduction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KE5Faa8NXJq1"},"outputs":[],"source":["# Reduce outliers\n","# Initialize TopicModeling with custom parameters\n","topic_modeling_outlier_reduction = TopicModeling(\n","    embedding_model_instance.model,\n","    umap_instance.umap_model,\n","    hdbscan_instance.hdbscan_model,\n","    top_n_words=5,\n","    nr_topics=25  # Dynamic number of topics\n",")\n","# Fit Model\n","topics, probabilities = topic_modeling_outlier_reduction.fit_transform(documents, umap_embeddings)\n","\n","# Reduce Outliers\n","new_topics = topic_modeling_outlier_reduction.reduce_outliers(documents, topics, probabilities=probabilities)\n","\n","# Update Model\n","topic_modeling_outlier_reduction.update_topics(documents, new_topics)\n","\n","# view Topics\n","topic_info_outliers = topic_modeling_outlier_reduction.topic_model.get_topic_info()\n","print(topic_info_outliers)\n","\n","topic_info_outliers = topic_info_outliers.drop(columns=['Representative_Docs'])\n","\n","# Save the modified DataFrame to a CSV file\n","topic_info_outliers.to_csv('/content/SIT723-CLR/Results/csv/topic_info_outliers-1.csv', index=False)\n","\n","# Evaluation\n","# Create an instance of TopicModelEvaluation\n","evaluation = TopicModelEvaluation(topic_model=topic_modeling_outlier_reduction.topic_model, dataset_path=tsv_directory)\n","\n","# Call evaluate_model on the instance\n","scores = evaluation.evaluate_model(topk=5)\n","\n","# Print the evaluation scores\n","print(\"\\nTopic Model Evaluation:\")\n","print(scores)"]},{"cell_type":"code","source":["# Evaluating individual topics\n","evaluation = TopicModelEvaluation(topic_model=topic_modeling_outlier_reduction.topic_model, dataset_path=tsv_directory)\n","individual_coherence_scores = evaluation.evaluate_individual_topics(topk=5)\n","\n","# Print individual coherence results\n","print(\"\\nIndividual Topic Coherence Scores (NPMI):\")\n","for topic_id, score in individual_coherence_scores.items():\n","    print(f\"Topic {topic_id}: {score}\")"],"metadata":{"id":"YcrEZzJBcmhe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Outlier reduction reduced topic coherence and diversity. We will continue with the original solution."],"metadata":{"id":"WmKn9LGrvYE6"}},{"cell_type":"markdown","source":["## Finding Optimal Model K"],"metadata":{"id":"LDjb5lM7UXZ3"}},{"cell_type":"code","source":["results = []\n","\n","for nr_topics in range(10, 31):\n","    topic_modeling_k_opt = TopicModeling(\n","        embedding_model_instance.model,\n","        umap_instance.umap_model,\n","        hdbscan_instance.hdbscan_model,\n","        top_n_words=5,\n","        nr_topics=nr_topics\n","    )\n","\n","    topics, probabilities = topic_modeling_k_opt.fit_transform(documents, umap_embeddings)\n","    evaluation = TopicModelEvaluation(topic_model=topic_modeling_k_opt.topic_model, dataset_path=tsv_directory)\n","    scores = evaluation.evaluate_model(topk=5)\n","\n","    results.append({'nr_topics': nr_topics, 'scores': scores})\n","\n","df = pd.DataFrame(results)\n","\n","print(df)"],"metadata":{"id":"Xj0lwGfeUXM4","executionInfo":{"status":"aborted","timestamp":1707373534170,"user_tz":-660,"elapsed":44,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores_df = df\n","\n","# Expand the 'scores' column into separate columns\n","scores_df = df['scores'].apply(pd.Series)\n","\n","# Concatenate the expanded scores back into the original DataFrame\n","scores_df = pd.concat([df.drop(columns='scores'), scores_df], axis=1)\n","\n","# Renaming the columns for clarity (if needed)\n","scores_df.rename(columns={'NPMI Coherence': 'NPMI_Coherence', 'Topic Diversity': 'Diversity'}, inplace=True)\n","\n","print(scores_df)"],"metadata":{"id":"HMBoozaNizcP","executionInfo":{"status":"aborted","timestamp":1707373534171,"user_tz":-660,"elapsed":44,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting up the aesthetics for seaborn plots\n","sns.set_theme(style=\"whitegrid\")\n","\n","# Creating a figure with two subplots (side by side)\n","fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n","\n","# Plotting NPMI Coherence\n","sns.lineplot(x='nr_topics', y='NPMI_Coherence', data=scores_df, marker='o', ax=axs[0])\n","axs[0].set_title('NPMI Coherence by Number of Topics')\n","axs[0].set_xlabel('Number of Topics')\n","axs[0].set_ylabel('NPMI Coherence')\n","\n","# Plotting Topic Diversity\n","sns.lineplot(x='nr_topics', y='Diversity', data=scores_df, marker='o', ax=axs[1])\n","axs[1].set_title('Topic Diversity by Number of Topics')\n","axs[1].set_xlabel('Number of Topics')\n","axs[1].set_ylabel('Diversity')\n","\n","# Adjusting layout for better spacing\n","plt.tight_layout()\n","\n","# Displaying the plots\n","plt.show()"],"metadata":{"id":"_30vXQwskWIZ","executionInfo":{"status":"aborted","timestamp":1707373534171,"user_tz":-660,"elapsed":44,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"naDsmQgzn4Fd"},"source":["## Fit and Visualise Hierarchichal Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qTywB2YU7Kw"},"outputs":[],"source":["# Create an instance of VisualisationAndEvaluation\n","visualisation = TopicVisualisation(topic_modeling.topic_model)\n","\n","# Hierarchical Topic Modeling\n","hierarchical_topics = topic_modeling.fit_hierarchical_topics(documents)\n","\n","# Visualize the hierarchical topics\n","hierarchy_vis = visualisation.visualise_hierarchy(hierarchical_topics)\n","display(hierarchy_vis)"]},{"cell_type":"markdown","metadata":{"id":"zTgesKGsEz5y"},"source":["## Merging Topics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gG9UA6syw5CX","executionInfo":{"status":"aborted","timestamp":1707373534172,"user_tz":-660,"elapsed":45,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"outputs":[],"source":["# # List of topics to merge\n","# topics_to_merge = [2, 7],\n","\n","# # Merging Topics\n","# topic_modeling.merge_topics(documents, topics_to_merge)\n","# # view Topics\n","# topic_info = topic_modeling.topic_model.get_topic_info()\n","# print(topic_info)\n","# # Create an instance of VisualisationAndEvaluation\n","# visualisation = TopicVisualisation(topic_modeling.topic_model)\n","\n","# # Hierarchical Topic Modeling\n","# hierarchical_topics = topic_modeling.fit_hierarchical_topics(documents)\n","\n","# # Visualize the hierarchical topics\n","# hierarchy_vis = visualisation.visualise_hierarchy(hierarchical_topics)\n","# display(hierarchy_vis)\n","# # Evaluation\n","# # Create an instance of TopicModelEvaluation\n","# evaluation = TopicModelEvaluation(topic_model=topic_modeling_outlier_reduction.topic_model, dataset_path=tsv_directory)\n","\n","# # Call evaluate_model on the instance\n","# scores = evaluation.evaluate_model(topk=5)\n","\n","# # Print the evaluation scores\n","# print(\"\\nTopic Model Evaluation:\")\n","# print(scores)\n","\n","# # Evaluating individual topics\n","# evaluation = TopicModelEvaluation(topic_model=topic_modeling_outlier_reduction.topic_model, dataset_path=tsv_directory)\n","# individual_coherence_scores = evaluation.evaluate_individual_topics(topk=5)\n","\n","# # Print individual coherence results\n","# print(\"\\nIndividual Topic Coherence Scores (NPMI):\")\n","# for topic_id, score in individual_coherence_scores.items():\n","#     print(f\"Topic {topic_id}: {score}\")"]},{"cell_type":"markdown","metadata":{"id":"FSBMoAApnnoS"},"source":["## Visualise Topics"]},{"cell_type":"markdown","metadata":{"id":"0fAUulRJnruN"},"source":["### Intertopic Distance Map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OqUNU9k8FKa"},"outputs":[],"source":["# Visualiser Initialisation\n","visualisation = TopicVisualisation(topic_modeling.topic_model)\n","\n","# Visualise intertopic distance map\n","topics_vis = visualisation.visualise_topics()\n","display(topics_vis)"]},{"cell_type":"markdown","metadata":{"id":"zkAa3__-nqzN"},"source":["### Barchart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dow6ikTk8GQS"},"outputs":[],"source":["# Visualise topic barcharts\n","barchart_vis = visualisation.visualise_barchart(top_n_topics=25)\n","display(barchart_vis)"]},{"cell_type":"markdown","metadata":{"id":"WURymiZtn82V"},"source":["### Topic Heatmap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGTX15h2Lf_V"},"outputs":[],"source":["# Create an instance of VisualisationAndEvaluation\n","visualisation = TopicVisualisation(topic_modeling.topic_model)\n","\n","# Visualize heatmap\n","heatmap_vis = visualisation.visualise_heatmap()\n","display(heatmap_vis)"]},{"cell_type":"code","source":["# Define the labels and sizes\n","labels = [\n","    'Weight Loss, Dieting, Nutrition\\n(2 Topics)',\n","    'Women and Children\\n(5 Topics)',\n","    'Physical Activity\\n(2 Topics)',\n","    'Behaviour Change\\n(3 Topics)',\n","    'Implementation, \\nModification, \\nAdaptation\\n(3 Topics)',\n","    'Substance Abuse\\n(2 Topics)',\n","    'Mental Health\\n(2 Topics)',\n","    'Other\\n(5 Topics)'\n","]\n","sizes = [2, 5, 2, 3, 3, 2, 2, 5]  # Number of topics\n","\n","# Define colors for each segment\n","colors = [\n","    '#E63946',  # Bright Red\n","    '#F1FAEE',  # Off White for contrast\n","    '#A8DADC',  # Powder Blue\n","    '#457B9D',  # Steel Blue\n","    '#1D3557',  # Prussian Blue\n","    '#F4A261',  # Sandy Brown\n","    '#2A9D8F',  # Teal Green\n","    '#E9C46A',  # Saffron\n","]\n","\n","# Create the treemap\n","plt.figure(figsize=(12, 8))\n","squarify.plot(sizes=sizes, label=labels, color=colors, alpha=0.7)\n","\n","plt.title('Treemap of Topic Themes')\n","plt.axis('off')  # Removes the axes\n","plt.show()"],"metadata":{"id":"9WlExtx02vIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdpjhV50n_vW"},"source":["## Fit and Visualise Dynamic Topic Model"]},{"cell_type":"code","source":["# Topic Labels Dictionary\n","topic_labels = {\n","    0: \"Topic 0: Physical Activity Interventions\",\n","    1: \"Topic 1: Social Behavioral Change Interventions\",\n","    2: \"Topic 2: Intervention Implementation Frameworks\",\n","    3: \"Topic 3: Stroke Rehabilitation Interventions\",\n","    4: \"Topic 4: Physical Activity Interventions\",\n","    5: \"Topic 5: Women's Health Interventions\",\n","    6: \"Topic 6: Smoking Cessation Interventions\",\n","    7: \"Topic 7: Mental Health Care\",\n","    8: \"Topic 8: Child Language Development Interventions\",\n","    9: \"Topic 9: Intervention Modification\",\n","    10: \"Topic 10: Behavior Change Theories\",\n","    11: \"Topic 11: Intervention Adaptation\",\n","    12: \"Topic 12: Behaviour Change Research Domains\",\n","    13: \"Topic 13: Patient Focussed Mental Health\",\n","    14: \"Topic 14: Child-focused Physical Ativity Interventions\",\n","    15: \"Topic 15: Alcohol Consumption Interventions\",\n","    16: \"Topic 16: Implementation of Education and Training\",\n","    17: \"Topic 17: Nutrition and Dieting Interventions\",\n","    18: \"Topic 18: Music Therapy Interventions\",\n","    19: \"Topic 19: Weight Loss Interventions \",\n","    20: \"Topic 20: Infants and Parents\",\n","    21: \"Topic 21: Child Focussed Habit Formation Interventions\",\n","    22: \"Topic 22: Veteran Focussed Interventions\",\n","    23: \"Topic 23: Cancer-Related Interventions\"\n","}\n"],"metadata":{"id":"FccVjtCmTpGc","executionInfo":{"status":"ok","timestamp":1707376385780,"user_tz":-660,"elapsed":369,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNlP18tLCbhX","executionInfo":{"status":"ok","timestamp":1707376453753,"user_tz":-660,"elapsed":55175,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}},"outputId":"4fd58d7f-8a13-43c3-e569-5a2772bbbf6d"},"outputs":[{"output_type":"stream","name":"stderr","text":["2it [00:54, 27.48s/it]\n"]}],"source":["# Create an instance of VisualisationAndEvaluation\n","visualisation = TopicVisualisation(topic_modeling.topic_model)\n","\n","# Dynamic Topic Modeling\n","# Ensure each document has a valid timestamp\n","timestamps = [data_loader.extract_year_from_filename(f) for f in os.listdir(data_loader.directory)]\n","timestamps = [t for t in timestamps if t is not None]\n","\n","# Perform dynamic topic modeling\n","topics_over_time = topic_modeling.topics_over_time(documents=documents, timestamps=timestamps, nr_bins=29)"]},{"cell_type":"code","source":["# Make sure this instance is of the updated class that includes the 'colormap' parameter\n","visualisation = TopicVisualisation(topic_modeling.topic_model)\n","\n","# Now call the method with the 'colormap' parameter\n","topics_over_time_vis = visualisation.visualise_topics_over_time(topics_over_time, top_n_topics=26, colormap='Rainbow')\n","topics_over_time_vis.show()"],"metadata":{"id":"5RyAWcq4nN89"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make sure this instance is of the updated class that includes the 'colormap' parameter\n","visualisation = TopicVisualisation(topic_modeling.topic_model)\n","\n","# Now call the method with the 'colormap' parameter\n","topics_over_time_vis = visualisation.visualise_topics_over_time(topics_over_time, top_n_topics=26, colormap='Viridis')\n","topics_over_time_vis.show()"],"metadata":{"id":"VwtIhYUJVRAF","executionInfo":{"status":"aborted","timestamp":1707373534177,"user_tz":-660,"elapsed":1103406,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Create an instance of VisualisationAndEvaluation\n","# visualisation = TopicVisualisation(topic_modeling_outlier_reduction.topic_model)\n","\n","# # Dynamic Topic Modeling\n","# # Ensure each document has a valid timestamp\n","# timestamps = [data_loader.extract_year_from_filename(f) for f in os.listdir(data_loader.directory)]\n","# timestamps = [t for t in timestamps if t is not None]\n","\n","# # Perform dynamic topic modeling\n","# topics_over_time = topic_modeling_outlier_reduction.topics_over_time(documents=documents, timestamps=timestamps, nr_bins=29)\n","# # Make sure this instance is of the updated class that includes the 'colormap' parameter\n","# visualisation = TopicVisualisation(topic_modeling_outlier_reduction.topic_model)\n","\n","# # Now call the method with the 'colormap' parameter\n","# topics_over_time_vis = visualisation.visualise_topics_over_time(topics_over_time, top_n_topics=26, colormap='Rainbow')\n","# topics_over_time_vis.show()"],"metadata":{"id":"tX8poQlQTjKN","executionInfo":{"status":"aborted","timestamp":1707373534177,"user_tz":-660,"elapsed":1103406,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nr382nXjMCI","executionInfo":{"status":"aborted","timestamp":1707373534177,"user_tz":-660,"elapsed":1103403,"user":{"displayName":"Storm Hiskens","userId":"15300871974107637162"}}},"outputs":[],"source":["!jupyter nbconvert --to pdf \"/content/SIT723-CLR/Content Analysis.ipynb\" --output-dir \"/content/SIT723-CLR/Results/Output\""]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"165qLRAlVy9GH1fM21kA_SCUAHbXZyb3l","timestamp":1706947955199},{"file_id":"1i3ocjmT3Rbl8OR26-TuI52dbdMOaseMc","timestamp":1706332683164}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}